# ML Service æ ‡å‡†æ¥å£

Reckit ä¸ TF Serving / ANN æœåŠ¡çš„æ ‡å‡†æ¥å£è®¾è®¡ï¼Œæä¾›ç»Ÿä¸€çš„æœºå™¨å­¦ä¹ æœåŠ¡å¯¹æ¥èƒ½åŠ›ã€‚

## è®¾è®¡ç›®æ ‡

- **ç»Ÿä¸€æ¥å£**ï¼šæä¾›ç»Ÿä¸€çš„ MLService æ¥å£ï¼Œæ”¯æŒ TF Servingã€TorchServeã€ANN ç­‰æœåŠ¡
- **åè®®æ”¯æŒ**ï¼šæ”¯æŒ gRPCï¼ˆTF Servingï¼‰å’Œ HTTP/REST
- **æ‰¹é‡é¢„æµ‹**ï¼šæ”¯æŒæ‰¹é‡è¯·æ±‚ï¼Œæé«˜æ€§èƒ½
- **é”™è¯¯å¤„ç†**ï¼šç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **é…ç½®åŒ–**ï¼šé€šè¿‡é…ç½®åˆ›å»ºæœåŠ¡å®ä¾‹

## æ ¸å¿ƒæ¥å£

### MLService

æ¥å£å®šä¹‰åœ¨ `core` åŒ…ï¼š

```go
import "github.com/rushteam/reckit/core"

type MLService interface {
    Predict(ctx context.Context, req *core.MLPredictRequest) (*core.MLPredictResponse, error)
    Health(ctx context.Context) error
    Close() error
}
```

## æœåŠ¡ç±»å‹

### 1. TensorFlow Serving

**åè®®**ï¼š
- gRPCï¼šç«¯å£ 8500ï¼ˆæ¨èï¼Œæ€§èƒ½æ›´å¥½ï¼‰
- REST APIï¼šç«¯å£ 8501ï¼ˆHTTP/JSONï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```go
import "github.com/rushteam/reckit/core"
import "github.com/rushteam/reckit/service"

// ä½¿ç”¨ REST API
var tfService core.MLService = service.NewTFServingClient(
    "http://localhost:8501",
    "my_model",
    service.WithTFServingVersion("1"),
    service.WithTFServingTimeout(30*time.Second),
)

// ä½¿ç”¨ gRPCï¼ˆæ¨èï¼‰
tfService = service.NewTFServingClient(
    "localhost:8500",
    "my_model",
    service.WithTFServingGRPC(),
    service.WithTFServingVersion("1"),
)

// æ‰¹é‡é¢„æµ‹
resp, err := tfService.Predict(ctx, &core.MLPredictRequest{
    Instances: [][]float64{
        {0.1, 0.2, 0.3, ...}, // å®ä¾‹ 1
        {0.4, 0.5, 0.6, ...}, // å®ä¾‹ 2
    },
})

// å¥åº·æ£€æŸ¥
err := tfService.Health(ctx)
```

### 2. TorchServe

**åè®®**ï¼š
- REST APIï¼šç«¯å£ 8080ï¼ˆæ¨ç†ï¼‰ã€8081ï¼ˆç®¡ç†ï¼‰
- gRPC APIï¼šç«¯å£ 7070ï¼ˆéœ€è¦é¢å¤–é…ç½®ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```go
import "github.com/rushteam/reckit/core"
import "github.com/rushteam/reckit/service"

// ä½¿ç”¨ REST API
var torchService core.MLService = service.NewTorchServeClient(
    "http://localhost:8080",
    "my_model",
    service.WithTorchServeVersion("1.0"),
    service.WithTorchServeTimeout(30*time.Second),
)

// æ‰¹é‡é¢„æµ‹
resp, err := torchService.Predict(ctx, &core.MLPredictRequest{
    Features: []map[string]float64{
        {"feature1": 0.1, "feature2": 0.2, ...}, // å®ä¾‹ 1
        {"feature1": 0.3, "feature2": 0.4, ...}, // å®ä¾‹ 2
    },
})

// æˆ–ä½¿ç”¨ Instancesï¼ˆç‰¹å¾å‘é‡ï¼‰
resp, err := torchService.Predict(ctx, &core.MLPredictRequest{
    Instances: [][]float64{
        {0.1, 0.2, 0.3, ...}, // å®ä¾‹ 1
        {0.4, 0.5, 0.6, ...}, // å®ä¾‹ 2
    },
})

// å¥åº·æ£€æŸ¥
err := torchService.Health(ctx)
```

### 3. ANN Service

**åè®®**ï¼šHTTP/REST

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```go
import "github.com/rushteam/reckit/core"
import "github.com/rushteam/reckit/service"

// æ³¨æ„ï¼šANNï¼ˆå‘é‡æ£€ç´¢ï¼‰åº”è¯¥ä½¿ç”¨ core.VectorService æ¥å£ï¼Œè€Œä¸æ˜¯ core.MLService
// è¯·å‚è€ƒ ext/vector/milvus æˆ– store.MemoryVectorService å®ç°å‘é‡æ£€ç´¢
//
// ç¤ºä¾‹ï¼š
// import milvus "github.com/rushteam/reckit/ext/vector/milvus"
// vectorService, _ := milvus.NewMilvusService("localhost:19530")
// result, err := vectorService.Search(ctx, &core.VectorSearchRequest{
//     Collection: "items",
//     Vector:     userVector,
//     TopK:       20,
//     Metric:     "cosine",
// })
```

## å·¥å‚æ–¹æ³•

ä½¿ç”¨å·¥å‚æ–¹æ³•æ ¹æ®é…ç½®åˆ›å»ºæœåŠ¡å®ä¾‹ï¼š

```go
import "github.com/rushteam/reckit/core"
import "github.com/rushteam/reckit/service"

// TF Serving é…ç½®
config := &service.ServiceConfig{
    Type:        service.ServiceTypeTFServing,
    Endpoint:    "http://localhost:8501",
    ModelName:   "my_model",
    ModelVersion: "1",
    Timeout:     30,
}

// TorchServe é…ç½®
torchConfig := &service.ServiceConfig{
    Type:        service.ServiceTypeTorchServe,
    Endpoint:    "http://localhost:8080",
    ModelName:   "my_model",
    ModelVersion: "1.0",
    Timeout:     30,
}

// åˆ›å»ºæœåŠ¡ï¼ˆè¿”å› core.MLServiceï¼‰
mlService, err := service.NewMLService(config)
torchService, err := service.NewMLService(torchConfig)
if err != nil {
    // å¤„ç†é”™è¯¯
}
defer mlService.Close(ctx)

// ä½¿ç”¨æœåŠ¡
resp, err := mlService.Predict(ctx, &core.MLPredictRequest{
    Instances: [][]float64{features},
})
```

## è¯·æ±‚/å“åº”æ ¼å¼

### MLPredictRequest

ç±»å‹å®šä¹‰åœ¨ `core` åŒ…ï¼š

```go
import "github.com/rushteam/reckit/core"

type MLPredictRequest struct {
    Instances     [][]float64              // ç‰¹å¾å‘é‡åˆ—è¡¨
    Features      []map[string]float64     // ç‰¹å¾å­—å…¸åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    ModelName     string                   // æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
    ModelVersion  string                   // æ¨¡å‹ç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰
    SignatureName string                   // ç­¾ååç§°ï¼ˆå¯é€‰ï¼‰
    Params        map[string]interface{}   // é¢å¤–å‚æ•°
}
```

### MLPredictResponse

ç±»å‹å®šä¹‰åœ¨ `core` åŒ…ï¼š

```go
type MLPredictResponse struct {
    Predictions  []float64    // é¢„æµ‹ç»“æœåˆ—è¡¨
    Outputs      interface{}  // åŸå§‹è¾“å‡ºï¼ˆå¯é€‰ï¼‰
    ModelVersion string       // æ¨¡å‹ç‰ˆæœ¬ï¼ˆå¦‚æœæœåŠ¡è¿”å›ï¼‰
}
```

## ä¸ç°æœ‰ä»£ç é›†æˆ

### 1. ä¸ model.RankModel é›†æˆ

```go
import (
    "github.com/rushteam/reckit/core"
    "github.com/rushteam/reckit/model"
    "github.com/rushteam/reckit/service"
)

// åˆ›å»º TF Serving æœåŠ¡
var tfService core.MLService = service.NewTFServingClient("http://localhost:8501", "rank_model")

// åŒ…è£…ä¸º RankModel
rpcModel := model.NewRPCModelFromService("tf_serving", tfService)

// åœ¨ Rank Node ä¸­ä½¿ç”¨
rankNode := &rank.RPCNode{
    Model: rpcModel,
}
```

### 2. ä¸ core.VectorDatabaseService é›†æˆ

```go
import (
    "github.com/rushteam/reckit/service"
)

// åˆ›å»º ANN æœåŠ¡å®¢æˆ·ç«¯
// æ³¨æ„ï¼šå‘é‡æ£€ç´¢åº”ä½¿ç”¨ core.VectorServiceï¼Œè¯·å‚è€ƒ ext/vector/milvus
// import milvus "github.com/rushteam/reckit/ext/vector/milvus"
// vectorService, _ := milvus.NewMilvusService("localhost:19530")
//
// åœ¨ recall.ANN ä¸­ä½¿ç”¨
// ann := &recall.ANN{
//     VectorService: vectorService,  // ç›´æ¥ä½¿ç”¨ core.VectorService
//     Collection:    "items",
//     TopK:          20,
// }
```

## åè®®è§„èŒƒ

### TF Serving REST API

**è¯·æ±‚æ ¼å¼**ï¼š
```json
POST /v1/models/{model_name}:predict
{
    "instances": [[f1, f2, f3, ...], ...],
    "signature_name": "serving_default"
}
```

**å“åº”æ ¼å¼**ï¼š
```json
{
    "predictions": [score1, score2, ...]
}
```

### TorchServe REST API

**è¯·æ±‚æ ¼å¼**ï¼š
```json
POST /predictions/{model_name}
{
    "data": [{"feature1": 0.1, "feature2": 0.2, ...}, ...]
}
```

æˆ–ä½¿ç”¨ç‰¹å¾å‘é‡ï¼š
```json
POST /predictions/{model_name}
{
    "data": [[0.1, 0.2, 0.3, ...], ...]
}
```

**å“åº”æ ¼å¼**ï¼ˆå–å†³äºæ¨¡å‹ Handlerï¼‰ï¼š
```json
[0.85, 0.72, ...]
```

æˆ–ï¼š
```json
{
    "prediction": 0.85
}
```

### ANN Service HTTP API

**è¯·æ±‚æ ¼å¼**ï¼š
```json
POST /v1/vector/search
{
    "collection": "items",
    "vectors": [[v1, v2, v3, ...], ...],
    "top_k": 10,
    "metric": "cosine"
}
```

**å“åº”æ ¼å¼**ï¼š
```json
{
    "results": [
        {
            "ids": [1, 2, 3, ...],
            "scores": [0.95, 0.92, 0.88, ...]
        }
    ]
}
```

## å®ç°çŠ¶æ€

### âœ… å·²å®ç°

- **TensorFlow Serving REST API**ï¼šå®Œå…¨æ”¯æŒ
- **TorchServe REST API**ï¼šå®Œå…¨æ”¯æŒ
- **ANN Service**ï¼šå®Œå…¨æ”¯æŒ

### âš ï¸ éƒ¨åˆ†å®ç°

- **TensorFlow Serving gRPC**ï¼šæ¥å£å·²å®šä¹‰ï¼Œéœ€è¦ protobuf ä¾èµ–ï¼ˆå½“å‰å›é€€åˆ° REST APIï¼‰

### ğŸ“ å¾…å®ç°

- **TorchServe gRPC**ï¼šéœ€è¦é¢å¤–é…ç½®å’Œ protobuf ä¾èµ–

## ä½¿ç”¨ç¤ºä¾‹

å®Œæ•´ç¤ºä¾‹ä»£ç è¯·å‚è€ƒï¼š`examples/ml_service/main.go`

è¿è¡Œç¤ºä¾‹ï¼š
```bash
go run ./examples/ml_service
```

## å‚è€ƒ

- [TensorFlow Serving æ–‡æ¡£](https://www.tensorflow.org/tfx/guide/serving)
- [TensorFlow Serving REST API](https://www.tensorflow.org/tfx/serving/api_rest)
- [TensorFlow Serving gRPC API](https://www.tensorflow.org/tfx/serving/api_rest)
- [TorchServe æ–‡æ¡£](https://pytorch.org/serve/)
- [TorchServe REST API](https://pytorch.org/serve/rest_api.html)